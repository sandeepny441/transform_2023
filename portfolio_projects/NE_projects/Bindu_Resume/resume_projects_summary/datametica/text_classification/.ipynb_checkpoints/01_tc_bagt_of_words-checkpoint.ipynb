{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff0e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d859b6b",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b818342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>customer_text</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>My payment got declined.</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>My payment got declined.</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>The system is too slow.</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>How do I backup my data?</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>My payment got declined.</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id             customer_text  severity\n",
       "0      10001  My payment got declined.       low\n",
       "1      10002  My payment got declined.  critical\n",
       "2      10003   The system is too slow.  critical\n",
       "3      10004  How do I backup my data?  critical\n",
       "4      10005  My payment got declined.      high"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate random ticket IDs\n",
    "ticket_ids = range(10001, 10001 + n_samples)\n",
    "\n",
    "# Dummy customer texts (just for illustration, in a real-world scenario these will be more diverse and meaningful)\n",
    "customer_texts = [\n",
    "    \"I can't log into my account.\",\n",
    "    \"The application is crashing frequently.\",\n",
    "    \"How do I reset my password?\",\n",
    "    \"The system is too slow.\",\n",
    "    \"Data is not syncing across devices.\",\n",
    "    \"I received a wrong bill.\",\n",
    "    \"The UI is not user-friendly.\",\n",
    "    \"My payment got declined.\",\n",
    "    \"I am facing issues with the new update.\",\n",
    "    \"How do I backup my data?\"\n",
    "]\n",
    "\n",
    "# Randomly assign customer texts to ticket IDs\n",
    "texts = [random.choice(customer_texts) for _ in range(n_samples)]\n",
    "\n",
    "# Severity levels\n",
    "severities = [\"low\", \"medium\", \"high\", \"critical\"]\n",
    "\n",
    "# Randomly assign severity levels to ticket IDs\n",
    "severity_labels = [random.choice(severities) for _ in range(n_samples)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"ticket_id\": ticket_ids,\n",
    "    \"customer_text\": texts,\n",
    "    \"severity\": severity_labels\n",
    "})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66ffae",
   "metadata": {},
   "source": [
    "# Text cleaning or Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2068d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator.DESKTOP-\n",
      "[nltk_data]     QSF3VEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DESKTOP-\n",
      "[nltk_data]     QSF3VEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator.DESKTOP-\n",
      "[nltk_data]     QSF3VEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: sampl text tag url\n",
      "Detected Language: sv\n",
      "Generated Bigrams: [('sampl', 'text'), ('text', 'tag'), ('tag', 'url')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "\n",
    "# Download necessary resources for NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 2. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 7. Remove HTML tags\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean, '', text)\n",
    "    \n",
    "    # 9. Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # 8. Remove special characters (excluding space)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 10. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 3. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 1. Stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 4. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # 5. Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # 11. Remove extra whitespaces\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    # 12. Sentence segmentation\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 13. Language detection (just an example, you might need a more robust solution)\n",
    "    lang = detect(text)\n",
    "    \n",
    "    # 14. Code switching detection (not implementing as it's quite involved and requires additional tools)\n",
    "    # 15. N-gram generation (example for bigrams; you can adjust for other n-grams)\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    \n",
    "    # 16. Noise reduction (not implementing as it's specific to the dataset and might need manual rules)\n",
    "    \n",
    "    # Rejoining cleaned tokens to form the cleaned text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text, lang, bigrams\n",
    "\n",
    "# Example:\n",
    "text = \"Your sample text with <html> tags </html> and https://example.com URLs.\"\n",
    "cleaned_text, detected_lang, generated_bigrams = preprocess_text(text)\n",
    "\n",
    "print(f\"Cleaned Text: {cleaned_text}\")\n",
    "print(f\"Detected Language: {detected_lang}\")\n",
    "print(f\"Generated Bigrams: {generated_bigrams}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec29224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>customer_text</th>\n",
       "      <th>severity</th>\n",
       "      <th>detected_lang</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>payment got declin</td>\n",
       "      <td>low</td>\n",
       "      <td>fr</td>\n",
       "      <td>[(payment, got), (got, declin)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>payment got declin</td>\n",
       "      <td>critical</td>\n",
       "      <td>fr</td>\n",
       "      <td>[(payment, got), (got, declin)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>system slow</td>\n",
       "      <td>critical</td>\n",
       "      <td>cs</td>\n",
       "      <td>[(system, slow)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>backup data</td>\n",
       "      <td>critical</td>\n",
       "      <td>id</td>\n",
       "      <td>[(backup, data)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>payment got declin</td>\n",
       "      <td>high</td>\n",
       "      <td>fr</td>\n",
       "      <td>[(payment, got), (got, declin)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id       customer_text  severity detected_lang  \\\n",
       "0      10001  payment got declin       low            fr   \n",
       "1      10002  payment got declin  critical            fr   \n",
       "2      10003         system slow  critical            cs   \n",
       "3      10004         backup data  critical            id   \n",
       "4      10005  payment got declin      high            fr   \n",
       "\n",
       "                           bigrams  \n",
       "0  [(payment, got), (got, declin)]  \n",
       "1  [(payment, got), (got, declin)]  \n",
       "2                 [(system, slow)]  \n",
       "3                 [(backup, data)]  \n",
       "4  [(payment, got), (got, declin)]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the function preprocess_text is already defined as above and df is our dataframe\n",
    "\n",
    "# Applying the function to our dataframe's \"complaints\" column\n",
    "df['customer_text'] = df['customer_text'].apply(lambda x: preprocess_text(x)[0])\n",
    "\n",
    "# If you're interested in capturing detected language and bigrams as well:\n",
    "df['detected_lang'] = df['customer_text'].apply(lambda x: preprocess_text(x)[1])\n",
    "df['bigrams'] = df['customer_text'].apply(lambda x: preprocess_text(x)[2])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c134f",
   "metadata": {},
   "source": [
    "# EDA for Supervised Text Classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9afc15f4",
   "metadata": {},
   "source": [
    "Label Distribution\n",
    "\n",
    "Text Length Distribution\n",
    "\n",
    "Most/Least Frequent Words\n",
    "\n",
    "Word Clouds for Each Label\n",
    "\n",
    "N-gram Analysis\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) Values\n",
    "\n",
    "Correlation Between Features\n",
    "\n",
    "Named Entity Recognition (NER)\n",
    "\n",
    "POS (Part-of-Speech) Tagging Analysis\n",
    "\n",
    "Outliers Detection\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Sample Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be81e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
